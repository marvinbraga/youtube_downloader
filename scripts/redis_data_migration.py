"""
Script Principal de MigraÃ§Ã£o JSON â†’ Redis com SeguranÃ§a MÃ¡xima
Implementa migraÃ§Ã£o robusta com validaÃ§Ã£o, batches e rollback automÃ¡tico

Autor: Claude Code Agent  
Data: 2025-08-26
VersÃ£o: 2.0.0 - FASE 2 Production Ready
"""

import asyncio
import json
import os
import sys
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

import redis.asyncio as redis
from loguru import logger

# Importar componentes do sistema
sys.path.append(str(Path(__file__).parent.parent))

from app.services.redis_connection import get_redis_client, init_redis
from app.services.redis_audio_manager import RedisAudioManager
from app.services.redis_video_manager import RedisVideoManager
from scripts.migration_backup_system import MigrationBackupSystem


class RedisDataMigrationManager:
    """
    Gerenciador completo de migraÃ§Ã£o de dados JSON para Redis
    Implementa seguranÃ§a mÃ¡xima com zero tolerÃ¢ncia a perda de dados
    """
    
    def __init__(self, 
                 data_dir: str = "E:\\python\\youtube_downloader\\data",
                 batch_size: int = 10,
                 validation_level: str = "strict"):
        
        self.data_dir = Path(data_dir)
        self.batch_size = batch_size
        self.validation_level = validation_level  # strict, normal, basic
        
        # Componentes do sistema
        self.backup_system = MigrationBackupSystem()
        self.redis_client: Optional[redis.Redis] = None
        self.audio_manager: Optional[RedisAudioManager] = None
        self.video_manager: Optional[RedisVideoManager] = None
        
        # Estado da migraÃ§Ã£o
        self.migration_id = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        self.migration_state = {
            'status': 'initialized',
            'current_phase': 'setup',
            'start_time': None,
            'end_time': None,
            'total_records': 0,
            'processed_records': 0,
            'failed_records': 0,
            'batch_results': [],
            'checkpoints': [],
            'rollback_points': []
        }
        
        # ConfiguraÃ§Ã£o de logs
        self._setup_migration_logging()
        
        logger.info(f"ğŸš€ MigraÃ§Ã£o Redis inicializada: {self.migration_id}")
    
    def _setup_migration_logging(self):
        """Configura sistema de logs detalhados para migraÃ§Ã£o"""
        log_dir = Path("E:\\python\\youtube_downloader\\logs\\migration")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        log_file = log_dir / f"migration_{self.migration_id}.log"
        
        # Configurar logger especÃ­fico para migraÃ§Ã£o
        logger.add(
            str(log_file),
            format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level} | {name}:{function}:{line} | {message}",
            level="DEBUG",
            retention="30 days",
            compression="zip"
        )
        
        logger.info(f"ğŸ“‹ Logs de migraÃ§Ã£o: {log_file}")
    
    async def execute_full_migration(self) -> Dict[str, Any]:
        """
        Executa migraÃ§Ã£o completa com todas as proteÃ§Ãµes de seguranÃ§a
        
        Returns:
            Resultado detalhado da migraÃ§Ã£o
        """
        logger.info("ğŸš€ Iniciando migraÃ§Ã£o completa JSON â†’ Redis")
        self.migration_state['start_time'] = time.time()
        
        try:
            # FASE 1: PreparaÃ§Ã£o e Backup
            await self._phase_1_preparation()
            
            # FASE 2: InicializaÃ§Ã£o Redis
            await self._phase_2_redis_initialization()
            
            # FASE 3: MigraÃ§Ã£o de Dados
            await self._phase_3_data_migration()
            
            # FASE 4: ValidaÃ§Ã£o Completa
            await self._phase_4_validation()
            
            # FASE 5: FinalizaÃ§Ã£o
            await self._phase_5_finalization()
            
            self.migration_state['status'] = 'completed'
            self.migration_state['end_time'] = time.time()
            
            # Resultado final
            migration_result = await self._generate_migration_report()
            
            logger.success("âœ… MigraÃ§Ã£o completa executada com sucesso!")
            return migration_result
            
        except Exception as e:
            logger.error(f"âŒ Falha crÃ­tica na migraÃ§Ã£o: {e}")
            
            # Executar rollback automÃ¡tico
            rollback_result = await self._execute_emergency_rollback()
            
            self.migration_state['status'] = 'failed'
            self.migration_state['end_time'] = time.time()
            
            return {
                'success': False,
                'error': str(e),
                'migration_id': self.migration_id,
                'rollback_result': rollback_result,
                'migration_state': self.migration_state
            }
    
    async def _phase_1_preparation(self):
        """FASE 1: PreparaÃ§Ã£o e criaÃ§Ã£o de backup completo"""
        logger.info("ğŸ“‹ FASE 1: PreparaÃ§Ã£o e Backup")
        self.migration_state['current_phase'] = 'preparation'
        
        # Criar backup prÃ©-migraÃ§Ã£o
        logger.info("ğŸ”„ Criando backup prÃ©-migraÃ§Ã£o...")
        backup_result = self.backup_system.create_pre_migration_backup()
        
        if not backup_result['success']:
            raise Exception(f"Falha no backup prÃ©-migraÃ§Ã£o: {backup_result.get('error')}")
        
        # Validar backup
        logger.info("ğŸ” Validando backup...")
        validation_result = self.backup_system.validate_backup_before_migration()
        
        if not validation_result['validation_success']:
            raise Exception("Backup nÃ£o passou na validaÃ§Ã£o - migraÃ§Ã£o interrompida")
        
        # Criar checkpoint inicial
        checkpoint_data = {
            'phase': 'preparation_complete',
            'backup_result': backup_result,
            'validation_result': validation_result
        }
        
        self.backup_system.create_incremental_checkpoint('preparation_complete', checkpoint_data)
        self.migration_state['checkpoints'].append('preparation_complete')
        
        logger.success("âœ… FASE 1: PreparaÃ§Ã£o concluÃ­da")
    
    async def _phase_2_redis_initialization(self):
        """FASE 2: InicializaÃ§Ã£o e teste da conexÃ£o Redis"""
        logger.info("ğŸ”Œ FASE 2: InicializaÃ§Ã£o Redis")
        self.migration_state['current_phase'] = 'redis_initialization'
        
        # Inicializar conexÃ£o Redis
        logger.info("Conectando ao Redis...")
        await init_redis()
        self.redis_client = await get_redis_client()
        
        # Testar conectividade
        await self.redis_client.ping()
        logger.info("âœ… ConexÃ£o Redis estabelecida")
        
        # Inicializar managers
        self.audio_manager = RedisAudioManager()
        self.video_manager = RedisVideoManager()
        
        await self.audio_manager.initialize()
        await self.video_manager.initialize()
        
        # Verificar estado inicial do Redis
        redis_info = await self.redis_client.info()
        logger.info(f"ğŸ“Š Redis Info: VersÃ£o {redis_info.get('redis_version')}")
        logger.info(f"ğŸ“Š MemÃ³ria usada: {redis_info.get('used_memory_human')}")
        
        # Criar checkpoint Redis
        checkpoint_data = {
            'phase': 'redis_initialized',
            'redis_info': {
                'version': redis_info.get('redis_version'),
                'memory': redis_info.get('used_memory_human'),
                'clients': redis_info.get('connected_clients')
            }
        }
        
        self.backup_system.create_incremental_checkpoint('redis_initialized', checkpoint_data)
        self.migration_state['checkpoints'].append('redis_initialized')
        
        logger.success("âœ… FASE 2: Redis inicializado")
    
    async def _phase_3_data_migration(self):
        """FASE 3: MigraÃ§Ã£o dos dados em batches com validaÃ§Ã£o"""
        logger.info("ğŸ“¤ FASE 3: MigraÃ§Ã£o de Dados")
        self.migration_state['current_phase'] = 'data_migration'
        
        # Migrar Ã¡udios
        await self._migrate_audio_data()
        
        # Migrar vÃ­deos  
        await self._migrate_video_data()
        
        logger.success("âœ… FASE 3: MigraÃ§Ã£o de dados concluÃ­da")
    
    async def _migrate_audio_data(self):
        """Migra dados de Ã¡udios em batches com validaÃ§Ã£o incremental"""
        logger.info("ğŸµ Migrando dados de Ã¡udios...")
        
        # Carregar dados JSON
        audios_file = self.data_dir / 'audios.json'
        if not audios_file.exists():
            logger.warning("âš ï¸ Arquivo audios.json nÃ£o encontrado")
            return
        
        with open(audios_file, 'r', encoding='utf-8') as f:
            audios_data = json.load(f)
        
        audios_list = audios_data.get('audios', [])
        total_audios = len(audios_list)
        
        logger.info(f"ğŸ“Š Total de Ã¡udios para migrar: {total_audios}")
        self.migration_state['total_records'] += total_audios
        
        # Processar em batches
        for i in range(0, total_audios, self.batch_size):
            batch = audios_list[i:i + self.batch_size]
            batch_number = (i // self.batch_size) + 1
            total_batches = (total_audios + self.batch_size - 1) // self.batch_size
            
            logger.info(f"ğŸ”„ Processando batch {batch_number}/{total_batches} ({len(batch)} Ã¡udios)")
            
            # Processar batch
            batch_result = await self._process_audio_batch(batch, batch_number)
            self.migration_state['batch_results'].append(batch_result)
            
            # Atualizar contadores
            self.migration_state['processed_records'] += batch_result['processed']
            self.migration_state['failed_records'] += batch_result['failed']
            
            # Criar checkpoint a cada 5 batches
            if batch_number % 5 == 0:
                checkpoint_data = {
                    'phase': f'audio_batch_{batch_number}',
                    'processed_records': self.migration_state['processed_records'],
                    'batch_results': self.migration_state['batch_results'][-5:]  # Ãšltimos 5 batches
                }
                
                self.backup_system.create_incremental_checkpoint(
                    f'audio_batch_{batch_number}', checkpoint_data
                )
            
            # Pausa entre batches para nÃ£o sobrecarregar
            await asyncio.sleep(0.1)
        
        logger.success(f"âœ… MigraÃ§Ã£o de Ã¡udios concluÃ­da: {self.migration_state['processed_records']} processados")
    
    async def _process_audio_batch(self, batch: List[Dict], batch_number: int) -> Dict[str, Any]:
        """Processa um batch de Ã¡udios com validaÃ§Ã£o"""
        batch_start_time = time.time()
        processed = 0
        failed = 0
        errors = []
        
        for audio_data in batch:
            try:
                # Validar estrutura do Ã¡udio
                if not self._validate_audio_structure(audio_data):
                    errors.append({
                        'audio_id': audio_data.get('id', 'unknown'),
                        'error': 'invalid_structure'
                    })
                    failed += 1
                    continue
                
                # Adicionar ao Redis
                success = await self.audio_manager.add_audio(audio_data)
                
                if success:
                    processed += 1
                    
                    # ValidaÃ§Ã£o incremental (strict mode)
                    if self.validation_level == 'strict':
                        retrieved = await self.audio_manager.get_audio(audio_data['id'])
                        if not self._compare_audio_data(audio_data, retrieved):
                            errors.append({
                                'audio_id': audio_data['id'],
                                'error': 'validation_mismatch'
                            })
                            failed += 1
                            processed -= 1
                else:
                    errors.append({
                        'audio_id': audio_data.get('id', 'unknown'),
                        'error': 'redis_add_failed'
                    })
                    failed += 1
                    
            except Exception as e:
                logger.error(f"Erro processando Ã¡udio {audio_data.get('id', 'unknown')}: {e}")
                errors.append({
                    'audio_id': audio_data.get('id', 'unknown'),
                    'error': str(e)
                })
                failed += 1
        
        batch_duration = time.time() - batch_start_time
        
        batch_result = {
            'batch_number': batch_number,
            'type': 'audio',
            'processed': processed,
            'failed': failed,
            'errors': errors,
            'duration_seconds': round(batch_duration, 2),
            'timestamp': time.time()
        }
        
        logger.info(f"ğŸ“Š Batch {batch_number}: {processed} OK, {failed} falhas, {batch_duration:.2f}s")
        
        return batch_result
    
    async def _migrate_video_data(self):
        """Migra dados de vÃ­deos"""
        logger.info("ğŸ¬ Migrando dados de vÃ­deos...")
        
        videos_file = self.data_dir / 'videos.json'
        if not videos_file.exists():
            logger.warning("âš ï¸ Arquivo videos.json nÃ£o encontrado")
            return
        
        with open(videos_file, 'r', encoding='utf-8') as f:
            videos_data = json.load(f)
        
        videos_list = videos_data.get('videos', [])
        total_videos = len(videos_list)
        
        logger.info(f"ğŸ“Š Total de vÃ­deos para migrar: {total_videos}")
        self.migration_state['total_records'] += total_videos
        
        # Processar vÃ­deos (normalmente sÃ£o poucos, pode processar todos de uma vez)
        for video_data in videos_list:
            try:
                # Validar estrutura
                if not self._validate_video_structure(video_data):
                    logger.warning(f"Estrutura invÃ¡lida para vÃ­deo: {video_data}")
                    self.migration_state['failed_records'] += 1
                    continue
                
                # Adicionar ao Redis
                success = await self.video_manager.add_video(video_data)
                
                if success:
                    self.migration_state['processed_records'] += 1
                    
                    # ValidaÃ§Ã£o incremental
                    if self.validation_level in ['strict', 'normal']:
                        retrieved = await self.video_manager.get_video(video_data['name'])
                        if not self._compare_video_data(video_data, retrieved):
                            logger.error(f"ValidaÃ§Ã£o falhou para vÃ­deo: {video_data['name']}")
                            self.migration_state['failed_records'] += 1
                            self.migration_state['processed_records'] -= 1
                else:
                    logger.error(f"Falha ao adicionar vÃ­deo: {video_data}")
                    self.migration_state['failed_records'] += 1
                    
            except Exception as e:
                logger.error(f"Erro processando vÃ­deo: {e}")
                self.migration_state['failed_records'] += 1
        
        logger.success("âœ… MigraÃ§Ã£o de vÃ­deos concluÃ­da")
    
    def _validate_audio_structure(self, audio_data: Dict) -> bool:
        """Valida estrutura de dados de Ã¡udio"""
        required_fields = ['id', 'title', 'youtube_id', 'url']
        
        for field in required_fields:
            if field not in audio_data:
                logger.warning(f"Campo obrigatÃ³rio ausente: {field}")
                return False
        
        # ValidaÃ§Ãµes adicionais
        if not isinstance(audio_data.get('keywords', []), list):
            return False
        
        if 'filesize' in audio_data and not isinstance(audio_data['filesize'], (int, float)):
            return False
        
        return True
    
    def _validate_video_structure(self, video_data: Dict) -> bool:
        """Valida estrutura de dados de vÃ­deo"""
        required_fields = ['name', 'path', 'type']
        
        for field in required_fields:
            if field not in video_data:
                logger.warning(f"Campo obrigatÃ³rio ausente: {field}")
                return False
        
        return True
    
    def _compare_audio_data(self, original: Dict, retrieved: Dict) -> bool:
        """Compara dados de Ã¡udio original vs recuperado"""
        if not retrieved:
            return False
        
        # Campos crÃ­ticos que devem ser idÃªnticos
        critical_fields = ['id', 'title', 'youtube_id', 'url']
        
        for field in critical_fields:
            if original.get(field) != retrieved.get(field):
                logger.warning(f"Mismatch no campo {field}: {original.get(field)} vs {retrieved.get(field)}")
                return False
        
        return True
    
    def _compare_video_data(self, original: Dict, retrieved: Dict) -> bool:
        """Compara dados de vÃ­deo original vs recuperado"""
        if not retrieved:
            return False
        
        critical_fields = ['name', 'path', 'type']
        
        for field in critical_fields:
            if original.get(field) != retrieved.get(field):
                logger.warning(f"Mismatch no campo {field}: {original.get(field)} vs {retrieved.get(field)}")
                return False
        
        return True
    
    async def _phase_4_validation(self):
        """FASE 4: ValidaÃ§Ã£o completa dos dados migrados"""
        logger.info("ğŸ” FASE 4: ValidaÃ§Ã£o Completa")
        self.migration_state['current_phase'] = 'validation'
        
        # Validar contadores
        await self._validate_record_counts()
        
        # Validar integridade dos dados
        await self._validate_data_integrity()
        
        # Validar funcionalidades crÃ­ticas
        await self._validate_critical_operations()
        
        logger.success("âœ… FASE 4: ValidaÃ§Ã£o completa")
    
    async def _validate_record_counts(self):
        """Valida se todos os registros foram migrados"""
        logger.info("ğŸ“Š Validando contadores de registros...")
        
        # Contar Ã¡udios no Redis
        redis_audio_count = await self.audio_manager.get_total_count()
        
        # Contar Ã¡udios no JSON
        audios_file = self.data_dir / 'audios.json'
        json_audio_count = 0
        
        if audios_file.exists():
            with open(audios_file, 'r', encoding='utf-8') as f:
                audios_data = json.load(f)
            json_audio_count = len(audios_data.get('audios', []))
        
        logger.info(f"ğŸ“Š Ãudios - JSON: {json_audio_count}, Redis: {redis_audio_count}")
        
        if json_audio_count != redis_audio_count:
            raise Exception(f"Mismatch na contagem de Ã¡udios: JSON={json_audio_count}, Redis={redis_audio_count}")
        
        # Contar vÃ­deos
        redis_video_count = await self.video_manager.get_total_count()
        
        videos_file = self.data_dir / 'videos.json'
        json_video_count = 0
        
        if videos_file.exists():
            with open(videos_file, 'r', encoding='utf-8') as f:
                videos_data = json.load(f)
            json_video_count = len(videos_data.get('videos', []))
        
        logger.info(f"ğŸ“Š VÃ­deos - JSON: {json_video_count}, Redis: {redis_video_count}")
        
        if json_video_count != redis_video_count:
            raise Exception(f"Mismatch na contagem de vÃ­deos: JSON={json_video_count}, Redis={redis_video_count}")
        
        logger.success("âœ… Contadores de registros validados")
    
    async def _validate_data_integrity(self):
        """Valida integridade dos dados migrados"""
        logger.info("ğŸ” Validando integridade dos dados...")
        
        # Validar sample de Ã¡udios (10%)
        all_audio_ids = await self.audio_manager.get_all_audio_ids()
        sample_size = max(1, len(all_audio_ids) // 10)  # 10% de sample
        sample_ids = all_audio_ids[:sample_size]
        
        logger.info(f"ğŸ” Validando sample de {sample_size} Ã¡udios...")
        
        for audio_id in sample_ids:
            audio_data = await self.audio_manager.get_audio(audio_id)
            if not audio_data:
                raise Exception(f"Ãudio nÃ£o encontrado no Redis: {audio_id}")
            
            # Validar estrutura
            if not self._validate_audio_structure(audio_data):
                raise Exception(f"Estrutura invÃ¡lida para Ã¡udio: {audio_id}")
        
        logger.success("âœ… Integridade dos dados validada")
    
    async def _validate_critical_operations(self):
        """Valida operaÃ§Ãµes crÃ­ticas do sistema"""
        logger.info("âš¡ Validando operaÃ§Ãµes crÃ­ticas...")
        
        # Testar busca por keywords
        search_results = await self.audio_manager.search_audios("davinci")
        logger.info(f"ğŸ” Teste de busca: {len(search_results)} resultados para 'davinci'")
        
        # Testar filtros
        filter_results = await self.audio_manager.filter_audios({'format': 'm4a'})
        logger.info(f"ğŸ” Teste de filtro: {len(filter_results)} Ã¡udios m4a")
        
        logger.success("âœ… OperaÃ§Ãµes crÃ­ticas validadas")
    
    async def _phase_5_finalization(self):
        """FASE 5: FinalizaÃ§Ã£o da migraÃ§Ã£o"""
        logger.info("ğŸ FASE 5: FinalizaÃ§Ã£o")
        self.migration_state['current_phase'] = 'finalization'
        
        # Criar checkpoint final
        final_checkpoint_data = {
            'phase': 'migration_complete',
            'migration_state': self.migration_state,
            'timestamp': time.time()
        }
        
        self.backup_system.create_incremental_checkpoint('migration_complete', final_checkpoint_data)
        self.migration_state['checkpoints'].append('migration_complete')
        
        logger.success("âœ… FASE 5: FinalizaÃ§Ã£o concluÃ­da")
    
    async def _generate_migration_report(self) -> Dict[str, Any]:
        """Gera relatÃ³rio completo da migraÃ§Ã£o"""
        total_duration = self.migration_state['end_time'] - self.migration_state['start_time']
        
        report = {
            'success': True,
            'migration_id': self.migration_id,
            'duration_seconds': round(total_duration, 2),
            'duration_minutes': round(total_duration / 60, 2),
            'total_records': self.migration_state['total_records'],
            'processed_records': self.migration_state['processed_records'],
            'failed_records': self.migration_state['failed_records'],
            'success_rate': round(
                (self.migration_state['processed_records'] / self.migration_state['total_records']) * 100, 2
            ) if self.migration_state['total_records'] > 0 else 0,
            'batches_processed': len(self.migration_state['batch_results']),
            'checkpoints_created': len(self.migration_state['checkpoints']),
            'validation_level': self.validation_level,
            'migration_state': self.migration_state,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        # Salvar relatÃ³rio
        report_dir = Path("E:\\python\\youtube_downloader\\reports\\migration")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        report_file = report_dir / f"migration_report_{self.migration_id}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.success(f"ğŸ“Š RelatÃ³rio de migraÃ§Ã£o salvo: {report_file}")
        
        return report
    
    async def _execute_emergency_rollback(self) -> Dict[str, Any]:
        """Executa rollback de emergÃªncia em caso de falha"""
        logger.warning("ğŸš¨ Executando rollback de emergÃªncia...")
        
        try:
            # Limpar dados Redis
            if self.redis_client:
                # Remover apenas keys relacionadas ao projeto
                keys = await self.redis_client.keys("youtube_downloader:*")
                if keys:
                    await self.redis_client.delete(*keys)
                    logger.info(f"ğŸ—‘ï¸ Removidas {len(keys)} chaves do Redis")
            
            rollback_result = {
                'success': True,
                'timestamp': time.time(),
                'keys_removed': len(keys) if 'keys' in locals() else 0,
                'message': 'Rollback de emergÃªncia executado com sucesso'
            }
            
            logger.success("âœ… Rollback de emergÃªncia concluÃ­do")
            return rollback_result
            
        except Exception as e:
            logger.error(f"âŒ Falha no rollback de emergÃªncia: {e}")
            return {
                'success': False,
                'error': str(e),
                'timestamp': time.time()
            }


# FunÃ§Ã£o principal para execuÃ§Ã£o
async def execute_migration(validation_level: str = "strict", batch_size: int = 10) -> Dict[str, Any]:
    """
    Executa migraÃ§Ã£o completa com configuraÃ§Ãµes especificadas
    
    Args:
        validation_level: NÃ­vel de validaÃ§Ã£o (strict, normal, basic)
        batch_size: Tamanho dos batches de processamento
        
    Returns:
        Resultado da migraÃ§Ã£o
    """
    migration_manager = RedisDataMigrationManager(
        batch_size=batch_size,
        validation_level=validation_level
    )
    
    return await migration_manager.execute_full_migration()


if __name__ == "__main__":
    # ExecuÃ§Ã£o de exemplo
    async def main():
        result = await execute_migration(validation_level="strict", batch_size=10)
        
        if result['success']:
            print(f"âœ… MigraÃ§Ã£o concluÃ­da com sucesso!")
            print(f"ğŸ“Š Processados: {result['processed_records']}/{result['total_records']}")
            print(f"â±ï¸ DuraÃ§Ã£o: {result['duration_minutes']:.1f} minutos")
            print(f"ğŸ“ˆ Taxa de sucesso: {result['success_rate']:.1f}%")
        else:
            print(f"âŒ MigraÃ§Ã£o falhou: {result.get('error')}")
            if 'rollback_result' in result:
                print(f"ğŸ”„ Rollback: {'âœ…' if result['rollback_result']['success'] else 'âŒ'}")
    
    asyncio.run(main())